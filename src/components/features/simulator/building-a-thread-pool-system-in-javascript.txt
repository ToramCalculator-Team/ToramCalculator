If you’ve been following our series on JavaScript concurrency, you’re probably excited to put all that knowledge into practice. Today, we’re going to build something really cool: thread pool system. Whether you’re processing large datasets, handling CPU-intensive tasks, or building a high-performance backend service, a well-designed thread pool can be a game-changer for your application’s performance.

JavaScript Concurrency and Parallel Programming: Article Series

Understanding JavaScript Concurrency: The Big Picture
Inside the JavaScript Runtime: Event Loop and Threading Model
Web Workers: Parallel Processing in the Browser
Node.js Worker Threads: Server-Side Parallel Processing
SharedArrayBuffer and Memory Management in JavaScript
Atomic Operations and Synchronization in JavaScript
Inter-Thread Communication Patterns in JavaScript: Building Robust Multi-Threading Systems
Building a Thread Pool System in JavaScript
Advanced Concurrency Patterns in JavaScript: Semaphore, Mutex, Read-Write Lock, Deadlock Prevention and ResourceManager
Real-time Data Processing with Parallel JavaScript: A Practical Guide
Browser and Node.js Differences in Concurrency
Why Do We Need a Thread Pool?
You’re building an image processing service that needs to handle multiple requests simultaneously. Without a thread pool, you might create a new worker for each request. That’s like hiring a new employee for each task and firing them immediately after — not very efficient, right?

A thread pool solves this by maintaining a “pool” of reusable workers. Think of it as having a team of employees ready to take on tasks as they come in. This approach gives us several benefits:

Reduced overhead from worker creation/destruction
Better resource utilization
Controlled parallelism
Simplified task management
Improved application stability
Let’s build the ThreadPool:

// thread-pool.js
const { Worker } = require('worker_threads');
const EventEmitter = require('events');
const os = require('os');

class ThreadPool extends EventEmitter {
  constructor(config) {
    super();
    this.validateConfig(config);

    this.config = {
      size: config.size || os.cpus().length,
      workerScript: config.workerScript,
      maxQueueSize: config.maxQueueSize || 1000,
      taskTimeout: config.taskTimeout || 30000,
      maxRetries: config.maxRetries || 3,
      monitorInterval: config.monitorInterval || 5000
    };

    this.workers = [];
    this.taskQueue = [];
    this.activeWorkers = new Map();
    this.workerMetrics = new Map();
    this.accepting = true;

    this.initialize();
  }

  validateConfig(config) {
    if (!config.workerScript) {
      throw new Error('Worker script path is required');
    }
    if (config.size !== undefined && (config.size < 1 || !Number.isInteger(config.size))) {
      throw new Error('Invalid pool size');
    }
  }

  initialize() {
    for (let i = 0; i < this.config.size; i++) {
      this.createWorker();
    }
    this.monitorInterval = setInterval(() => this.monitorPoolHealth(), this.config.monitorInterval);
  }

  createWorker() {
    const worker = new Worker(this.config.workerScript);

    this.workerMetrics.set(worker, {
      tasksCompleted: 0,
      errors: 0,
      avgProcessingTime: 0,
      lastActive: Date.now()
    });

    worker.on('message', (result) => {
      this.handleTaskCompletion(worker, result);
    });

    worker.on('error', (error) => {
      this.handleWorkerError(worker, error);
    });

    worker.on('exit', (code) => {
      if (code !== 0) {
        this.handleWorkerExit(worker);
      }
    });

    this.workers.push(worker);
    return worker;
  }

  async submitTask(task) {
    if (!this.accepting) {
      throw new Error('Thread pool is shutting down');
    }

    return new Promise((resolve, reject) => {
      const taskWrapper = {
        id: Date.now() + Math.random(),
        task,
        resolve,
        reject,
        timestamp: Date.now(),
        retriesLeft: this.config.maxRetries
      };

      const availableWorker = this.getAvailableWorker();
      if (availableWorker) {
        this.assignTask(availableWorker, taskWrapper);
      } else {
        this.taskQueue.push(taskWrapper);
        if (this.taskQueue.length > this.config.maxQueueSize) {
          this.emit('queue-full', this.taskQueue.length);
        }
      }
    });
  }

  getAvailableWorker() {
    return this.workers.find(worker => !this.activeWorkers.has(worker));
  }

  assignTask(worker, taskWrapper) {
    const { id, task } = taskWrapper;
    this.activeWorkers.set(worker, taskWrapper);

    const timeoutId = setTimeout(() => {
      if (this.activeWorkers.has(worker)) {
        const taskWrapper = this.activeWorkers.get(worker);
        if (taskWrapper) {
          taskWrapper.reject(new Error('Task timeout'));
          this.activeWorkers.delete(worker);
          this.processNextTask();
        }
      }
    }, this.config.taskTimeout);

    taskWrapper.timeoutId = timeoutId;

    try {
      worker.postMessage({ id, task });
    } catch (error) {
      clearTimeout(timeoutId);
      this.activeWorkers.delete(worker);
      taskWrapper.reject(error);
      this.processNextTask();
    }
  }

  handleTaskCompletion(worker, { id, result, error }) {
    const taskWrapper = this.activeWorkers.get(worker);
    if (!taskWrapper) {
      // Worker completed a task but is no longer active
      // This is not an error condition, just ignore the result
      return;
    }

    clearTimeout(taskWrapper.timeoutId);
    this.updateWorkerMetrics(worker, error ? 'error' : 'success');

    this.activeWorkers.delete(worker);

    if (error) {
      if (taskWrapper.retriesLeft > 0) {
        taskWrapper.retriesLeft--;
        this.taskQueue.unshift(taskWrapper);
      } else {
        taskWrapper.reject(new Error(error));
      }
    } else {
      taskWrapper.resolve(result);
    }

    this.processNextTask();
  }

  handleWorkerError(worker, error) {
    const metrics = this.workerMetrics.get(worker);
    if (metrics) {
      metrics.errors++;
    }

    const taskWrapper = this.activeWorkers.get(worker);
    if (taskWrapper) {
      clearTimeout(taskWrapper.timeoutId);

      if (taskWrapper.retriesLeft > 0) {
        taskWrapper.retriesLeft--;
        this.taskQueue.unshift(taskWrapper);
      } else {
        taskWrapper.reject(error);
      }

      this.activeWorkers.delete(worker);
    }

    this.replaceWorker(worker);
    this.processNextTask();
  }

  handleWorkerExit(worker) {
    const index = this.workers.indexOf(worker);
    if (index !== -1) {
      this.workers.splice(index, 1);
      this.workerMetrics.delete(worker);

      if (this.accepting) {
        const newWorker = this.createWorker();
        this.workers.splice(index, 0, newWorker);
      }
    }
  }

  replaceWorker(worker) {
    const index = this.workers.indexOf(worker);
    if (index !== -1) {
      this.workers.splice(index, 1);
      this.workerMetrics.delete(worker);
      worker.terminate().catch(() => {});

      if (this.accepting) {
        const newWorker = this.createWorker();
        this.workers.splice(index, 0, newWorker);
      }
    }
  }

  processNextTask() {
    if (this.taskQueue.length === 0) return;

    const availableWorker = this.getAvailableWorker();
    if (availableWorker) {
      const nextTask = this.taskQueue.shift();
      this.assignTask(availableWorker, nextTask);
    }
  }

  updateWorkerMetrics(worker, status) {
    const metrics = this.workerMetrics.get(worker);
    if (!metrics) return;

    if (status === 'success') {
      metrics.tasksCompleted++;
    } else if (status === 'error') {
      metrics.errors++;
    }
    metrics.lastActive = Date.now();
  }

  monitorPoolHealth() {
    const metrics = {
      activeWorkers: this.activeWorkers.size,
      queueLength: this.taskQueue.length,
      workerMetrics: Array.from(this.workerMetrics.entries())
        .map(([worker, metrics]) => ({
          workerId: this.workers.indexOf(worker),
          ...metrics
        }))
    };
    this.emit('metrics', metrics);
  }

  async shutdown() {
    this.accepting = false;

    // Wait for active tasks to complete
    const activePromises = Array.from(this.activeWorkers.values())
      .map(task => new Promise(resolve => {
        const originalResolve = task.resolve;
        task.resolve = (result) => {
          originalResolve(result);
          resolve();
        };
      }));

    await Promise.all(activePromises);

    // Clean up intervals and workers
    clearInterval(this.monitorInterval);

    await Promise.all(this.workers.map(worker => {
      try {
        return worker.terminate();
      } catch (error) {
        return Promise.resolve();
      }
    }));

    this.workers = [];
    this.workerMetrics.clear();
    this.activeWorkers.clear();
    this.taskQueue = [];
  }
}

module.exports = ThreadPool;
When you first create a new ThreadPool, it’s like setting up a workplace. You provide a configuration that specifies important details like how many workers you want (size), what tasks they’ll be doing (workerScript), and some workplace policies like how long tasks can take (taskTimeout) and how many retry attempts are allowed (maxRetries).

The first thing that happens in the code is validation, it makes sure you’ve provided the essential information, particularly the workerScript (which is like the instruction manual for workers). If the configuration looks good, it starts the initialization process.

During initialization, the pool creates your requested number of workers. Each worker is created using NodeJS Worker Threads. Think of this like hiring employees, each one gets set up with their own performance metrics tracking:

How many tasks they’ve completed
How many errors they’ve had
How fast they typically work
When they last did something
Now, let’s look at how work actually flows through the system. When someone submits a task using submitTask(), here’s what happens.

First, the system wraps your task in a package that includes:

A unique ID (like an order number)
The actual work to be done
When it was submitted
How many more tries it gets if it fails
Then comes the interesting part, the system looks for an available worker using getAvailableWorker(). It’s like looking around the workplace to see who’s free. If it finds someone available, great! The task gets assigned right away through assignTask(). If everyone’s busy, the task goes into a queue to wait its turn.

When a task gets assigned to a worker, a few things happen simultaneously:

The task gets marked as active for that worker
A timeout clock starts ticking (like setting an alarm)
The actual work gets sent to the worker using postMessage
The system then waits for one of three things to happen.

The task completes successfully (handleTaskCompletion):

The timer is stopped
The worker’s stats are updated
The result is sent back to whoever submitted the task
The worker becomes available for new tasks
The task fails (handleWorkerError):

If it has retries left, it goes back to the front of the queue
If no retries left, the error is reported back
The worker gets replaced with a fresh one (like sending someone home and bringing in a replacement)
The task times out:

The task is cancelled
Depending on retries, it might try again
The system ensures the worker is ready for new work
Throughout all this, there’s a health monitoring system running every few seconds (monitorPoolHealth). It’s constantly checking:

How many workers are currently busy
How long the queue of waiting tasks is
How each worker is performing
When it’s time to shut everything down (shutdown), the system:

Stops accepting new tasks
Lets current tasks finish up
Cleans up all the workers
Clears out any remaining queued tasks
The really clever part is how it handles problems. If a worker crashes or starts having issues, the replaceWorker function automatically swaps them out with a fresh worker, maintaining the pool’s capacity. It’s like having a self-healing system that keeps everything running smoothly even when things go wrong.

All of this happens while maintaining a balance between performance and reliability. The queuing system ensures we don’t overwhelm our resources, the retry mechanism handles failures gracefully, and the monitoring system keeps us informed about how everything is running.

This implementation is particularly useful in scenarios where you need to process many tasks concurrently, like handling multiple user requests in a web server, processing large amounts of data, or running complex calculations across multiple threads. It ensures efficient resource usage while maintaining system stability and reliability.

Now let me create a corresponding worker script template:

// worker.js
const { parentPort } = require('worker_threads');

// Simulate CPU-intensive work
function simulateWork(duration) {
  const start = Date.now();
  while (Date.now() - start < duration) {
    // Busy wait to simulate CPU work
    for (let i = 0; i < 1000000; i++) {}
  }
}

// Process tasks
parentPort.on('message', async ({ id, task }) => {
  try {
    // Simulate random delay
    const processingTime = task.expectedRuntime || Math.random() * 1000;

    // Simulate task processing
    if (task.complexity) {
      simulateWork(processingTime * task.complexity);
    } else {
      await new Promise(resolve => setTimeout(resolve, processingTime));
    }

    // Simulate random failures
    if (task.shouldFail && Math.random() < 0.8) { // 80% chance of failure if shouldFail is true
      throw new Error(`Task ${task.data} failed`);
    }

    // Send successful result
    parentPort.postMessage({
      id,
      result: `Completed ${task.data} in ${processingTime}ms`
    });
  } catch (error) {
    // Send error result
    parentPort.postMessage({
      id,
      error: error.message
    });
  }
});
Now, let’s create the main execution file to run the thread pool battle tests:

// main.js
const ThreadPool = require('./thread-pool');
const path = require('path');

// Utility to format numbers
const formatNumber = (num) => new Intl.NumberFormat().format(num);

// Utility to track execution time
class Timer {
  constructor() {
    this.startTime = Date.now();
  }

  elapsed() {
    return Date.now() - this.startTime;
  }

  reset() {
    this.startTime = Date.now();
  }
}

// Create complex tasks that simulate real work
function createComplexTask(id, complexity = 1) {
  return {
    data: `Task ${id}`,
    complexity,
    expectedRuntime: Math.floor(Math.random() * 1000) * complexity,
    shouldFail: Math.random() < 0.1 // 10% chance of failure
  };
}

async function runBattleTest() {
  console.log('Starting Thread Pool Battle Test');

  const timer = new Timer();
  const pool = new ThreadPool({
    workerScript: path.join(__dirname, 'worker.js'),
    size: 4,
    maxQueueSize: 1000,
    taskTimeout: 5000,
    maxRetries: 3
  });

  // Track metrics
  let stats = {
    totalTasks: 0,
    completedTasks: 0,
    failedTasks: 0,
    retries: 0,
    totalTime: 0
  };

  // Monitor pool health
  pool.on('metrics', (metrics) => {
    console.log('Pool Status:', {
      activeWorkers: metrics.activeWorkers,
      queueLength: metrics.queueLength,
      completedTasks: stats.completedTasks,
      failedTasks: stats.failedTasks,
      retries: stats.retries,
      uptime: `${Math.floor(timer.elapsed() / 1000)}s`
    });
  });

  pool.on('queue-full', (size) => {
    console.log(`Queue reached capacity: ${size} tasks`);
  });

  try {
    console.log('Test Scenarios:');
    console.log('1. Basic Task Processing');
    console.log('2. Priority Handling');
    console.log('3. Error Recovery');
    console.log('4. Heavy Load Test');
    console.log('5. Mixed Workload\n');

    // 1. Basic Task Processing
    console.log('Running Basic Task Processing...');
    timer.reset();
    const basicTasks = Array(20).fill().map((_, i) => createComplexTask(i));
    stats.totalTasks += basicTasks.length;

    const basicResults = await Promise.allSettled(
      basicTasks.map(task => pool.submitTask(task))
    );

    stats.completedTasks += basicResults.filter(r => r.status === 'fulfilled').length;
    stats.failedTasks += basicResults.filter(r => r.status === 'rejected').length;

    console.log(`✅ Basic tasks completed in ${timer.elapsed()}ms`);

    // 2. Priority Tasks
    console.log('Testing Priority Handling...');
    timer.reset();

    const priorityTasks = [
      { task: createComplexTask('HIGH-1'), priority: 1 },
      { task: createComplexTask('LOW-1'), priority: 3 },
      { task: createComplexTask('HIGH-2'), priority: 1 },
      { task: createComplexTask('MED-1'), priority: 2 },
      { task: createComplexTask('LOW-2'), priority: 3 }
    ];

    stats.totalTasks += priorityTasks.length;

    const priorityResults = await Promise.allSettled(
      priorityTasks.map(({ task, priority }) =>
        pool.submitTask(task, { priority })
      )
    );

    stats.completedTasks += priorityResults.filter(r => r.status === 'fulfilled').length;
    stats.failedTasks += priorityResults.filter(r => r.status === 'rejected').length;

    console.log(`✅ Priority tasks completed in ${timer.elapsed()}ms`);

    // 3. Error Recovery
    console.log('Testing Error Recovery...');
    timer.reset();

    const errorTasks = Array(10).fill().map((_, i) => ({
      data: `ErrorTask ${i}`,
      shouldFail: true
    }));

    stats.totalTasks += errorTasks.length;

    const errorResults = await Promise.allSettled(
      errorTasks.map(task => pool.submitTask(task))
    );

    stats.completedTasks += errorResults.filter(r => r.status === 'fulfilled').length;
    stats.failedTasks += errorResults.filter(r => r.status === 'rejected').length;
    stats.retries += errorTasks.length * 2; // Assuming average 2 retries per failed task

    console.log(`✅ Error recovery test completed in ${timer.elapsed()}ms`);

    // 4. Heavy Load Test
    console.log('Running Heavy Load Test...');
    timer.reset();

    const heavyTasks = Array(100).fill().map((_, i) =>
      createComplexTask(i, Math.random() * 3) // Random complexity
    );

    stats.totalTasks += heavyTasks.length;

    const heavyResults = await Promise.allSettled(
      heavyTasks.map(task => pool.submitTask(task))
    );

    stats.completedTasks += heavyResults.filter(r => r.status === 'fulfilled').length;
    stats.failedTasks += heavyResults.filter(r => r.status === 'rejected').length;

    console.log(`✅ Heavy load test completed in ${timer.elapsed()}ms`);

    // 5. Mixed Workload
    console.log('Testing Mixed Workload...');
    timer.reset();

    const mixedTasks = [];
    // High priority, complex tasks
    mixedTasks.push(...Array(5).fill().map((_, i) => ({
      task: createComplexTask(`HIGH-${i}`, 2),
      priority: 1
    })));
    // Medium priority, normal tasks
    mixedTasks.push(...Array(10).fill().map((_, i) => ({
      task: createComplexTask(`MED-${i}`, 1),
      priority: 2
    })));
    // Low priority, simple tasks
    mixedTasks.push(...Array(15).fill().map((_, i) => ({
      task: createComplexTask(`LOW-${i}`, 0.5),
      priority: 3
    })));

    stats.totalTasks += mixedTasks.length;

    const mixedResults = await Promise.allSettled(
      mixedTasks.map(({ task, priority }) =>
        pool.submitTask(task, { priority })
      )
    );

    stats.completedTasks += mixedResults.filter(r => r.status === 'fulfilled').length;
    stats.failedTasks += mixedResults.filter(r => r.status === 'rejected').length;

    console.log(`✅ Mixed workload test completed in ${timer.elapsed()}ms`);

    // Final Statistics
    console.log('Final Statistics:');
    console.log('--------------------');
    console.log(`Total Tasks: ${formatNumber(stats.totalTasks)}`);
    console.log(`Completed Tasks: ${formatNumber(stats.completedTasks)}`);
    console.log(`Failed Tasks: ${formatNumber(stats.failedTasks)}`);
    console.log(`Retry Attempts: ${formatNumber(stats.retries)}`);
    console.log(`Success Rate: ${((stats.completedTasks / stats.totalTasks) * 100).toFixed(2)}%`);
    console.log(`Total Test Time: ${(timer.elapsed() / 1000).toFixed(2)}s`);

    // Graceful Shutdown
    console.log('Initiating graceful shutdown...');
    await pool.shutdown();
    console.log('✅ Pool shut down successfully');

  } catch (error) {
    console.error('❌ Test failed:', error);
    await pool.shutdown();
  }
}

// Run the battle test
console.log('Starting battle test...');
runBattleTest().catch(console.error);
After you run these tests, you should see something like this as output:

Starting battle test...
Starting Thread Pool Battle Test
Test Scenarios:
1. Basic Task Processing
2. Priority Handling
3. Error Recovery
4. Heavy Load Test
5. Mixed Workload

Running Basic Task Processing...
✅ Basic tasks completed in 3245ms
Testing Priority Handling...
✅ Priority tasks completed in 933ms
Testing Error Recovery...
Pool Status: {
  activeWorkers: 4,
  queueLength: 3,
  completedTasks: 24,
  failedTasks: 1,
  retries: 0,
  uptime: '0s'
}
✅ Error recovery test completed in 3031ms
Running Heavy Load Test...
Pool Status: {
  activeWorkers: 4,
  queueLength: 89,
  completedTasks: 31,
  failedTasks: 4,
  retries: 20,
  uptime: '2s'
}
Pool Status: {
  activeWorkers: 4,
  queueLength: 74,
  completedTasks: 31,
  failedTasks: 4,
  retries: 20,
  uptime: '7s'
}

.......

✅ Heavy load test completed in 42995ms
Testing Mixed Workload...
✅ Mixed workload test completed in 3971ms
Final Statistics:
--------------------
Total Tasks: 165
Completed Tasks: 151
Failed Tasks: 14
Retry Attempts: 20
Success Rate: 91.52%
Total Test Time: 4.04s
Initiating graceful shutdown...
✅ Pool shut down successfully
Understanding the Thread Pool Architecture
Our thread pool implementation follows a robust architecture with several key components:

Pool Manager: The core class that maintains the worker pool and handles task distribution

Task Queue: A FIFO queue for managing incoming tasks when all workers are busy

Worker Management: Handles worker lifecycle, including creation, monitoring, and recovery

Health Monitoring: Tracks pool metrics and worker performance

Error Recovery: Implements strategies for handling various failure scenarios

Best Practices for Production Use
Right-size Your Pool

Start with number of CPU cores - 1 workers
Monitor and adjust based on your workload
Consider your application’s memory constraints
Implement Proper Monitoring

Set up alerts for queue buildup
Monitor worker health and performance
Track error rates and recovery events
Handle Graceful Shutdown

process.on('SIGTERM', async () => {
    console.log('Shutting down thread pool...');
    await pool.shutdown();
    process.exit(0);
});
Optimize Task Distribution

Group small tasks together
Split large tasks into smaller chunks
Consider task priorities if needed
Performance Optimization Tips
Task Batching When dealing with many small tasks, batch them together to reduce overhead:

async function batchProcessor(items, batchSize = 100) {
    const batches = [];
    for (let i = 0; i < items.length; i += batchSize) {
        batches.push(items.slice(i, i + batchSize));
    }
    
    return Promise.all(batches.map(batch => pool.submitTask({
        type: 'process',
        data: batch
    })));
}
Memory Management

Use TypedArrays for large data transfers
Implement proper cleanup in workers
Monitor memory usage patterns
Wrapping Up
Building a thread pool system is about more than just distributing tasks across workers. It’s about creating a reliable, performant, and maintainable system that can handle real-world challenges.

The implementation we’ve covered today provides a solid foundation that you can build upon based on your specific needs. Remember to:

Monitor your pool’s performance in production
Adjust pool size and configuration based on metrics
Implement proper error handling and recovery
Plan for graceful degradation under heavy load
In the next article, we’ll dive into advanced concurrency patterns that you can implement on top of this thread pool system. Stay tuned!